{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.initializers import Constant\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.layers import *\n",
    "from keras import regularizers\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_files_to_df(path):\n",
    "    contents = []\n",
    "    files = glob.glob(path)\n",
    "    for file in files:\n",
    "        with open(file) as f:\n",
    "            contents.append(f.read())\n",
    "    return pd.DataFrame(contents, columns=['text'])\n",
    "\n",
    "def remove_stopwords(input_text):\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    # Some words which might indicate a certain sentiment are kept via a whitelist\n",
    "    whitelist = [\"n't\", \"not\", \"no\"]\n",
    "    words = input_text.split() \n",
    "    clean_words = [word for word in words if (word not in stopwords_list or word in whitelist) and len(word) > 1] \n",
    "    return \" \".join(clean_words) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing data...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "\n",
    "### prepare data\n",
    "print('preparing data...')\n",
    "n_data_train = read_files_to_df('group34/data/train/neg/*.txt')\n",
    "p_data_train = read_files_to_df('group34/data/train/pos/*.txt')\n",
    "train_data = n_data_train.append(p_data_train, ignore_index=True)\n",
    "train_data.text = train_data.text.apply(remove_stopwords)\n",
    "\n",
    "n_labels = [0] * len(n_data_train)\n",
    "p_labels = [1] * len(p_data_train)\n",
    "labels = n_labels + p_labels\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_data.text, labels, test_size=0.20, random_state=SEED)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NB_WORDS = 6000\n",
    "tk = Tokenizer(num_words=NB_WORDS,\n",
    "               filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "               lower=True,\n",
    "               split=\" \")\n",
    "tk.fit_on_texts(X_train)\n",
    "\n",
    "X_train_seq = tk.texts_to_sequences(X_train)\n",
    "X_val_seq = tk.texts_to_sequences(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq_lengths = X_train.apply(lambda x: len(x.split(' ')))\n",
    "\n",
    "MAX_LEN = seq_lengths.max()\n",
    "X_train_seq_padded = pad_sequences(X_train_seq, maxlen=MAX_LEN)\n",
    "X_val_seq_padded = pad_sequences(X_val_seq, maxlen=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, None, 128)         768000    \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, None, 64)          41216     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_5 (Glob (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 20)                1300      \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 810,537\n",
      "Trainable params: 810,537\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embed_size = 128\n",
    "model = models.Sequential()\n",
    "model.add(Embedding(NB_WORDS, embed_size))\n",
    "model.add(Bidirectional(LSTM(32, return_sequences = True)))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dense(20, activation=\"relu\"))\n",
    "model.add(Dropout(0.05))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/8\n",
      " - 504s - loss: 0.4333 - acc: 0.8030 - val_loss: 0.2871 - val_acc: 0.8816\n",
      "Epoch 2/8\n",
      " - 491s - loss: 0.2293 - acc: 0.9123 - val_loss: 0.2868 - val_acc: 0.8798\n",
      "Epoch 3/8\n",
      " - 488s - loss: 0.1660 - acc: 0.9396 - val_loss: 0.3226 - val_acc: 0.8794\n",
      "Epoch 4/8\n",
      " - 482s - loss: 0.1199 - acc: 0.9597 - val_loss: 0.3585 - val_acc: 0.8734\n",
      "Epoch 5/8\n",
      " - 483s - loss: 0.0788 - acc: 0.9765 - val_loss: 0.4330 - val_acc: 0.8680\n",
      "Epoch 6/8\n",
      " - 484s - loss: 0.0637 - acc: 0.9803 - val_loss: 0.4422 - val_acc: 0.8648\n",
      "Epoch 7/8\n",
      " - 483s - loss: 0.0404 - acc: 0.9886 - val_loss: 0.5230 - val_acc: 0.8660\n",
      "Epoch 8/8\n",
      " - 484s - loss: 0.0373 - acc: 0.9890 - val_loss: 0.5894 - val_acc: 0.8636\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "batch_size = 100\n",
    "epochs = 8\n",
    "fft = model.fit(X_train_seq_padded,y_train, batch_size=batch_size, epochs=epochs, \n",
    "          validation_data=(X_val_seq_padded, y_val), \n",
    "          verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, None, 300)         1800000   \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, None, 64)          85248     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_3 (Glob (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 20)                1300      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 1,886,569\n",
      "Trainable params: 1,886,569\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/8\n",
      " - 889s - loss: 0.4019 - acc: 0.8161 - val_loss: 0.2915 - val_acc: 0.8792\n",
      "Epoch 2/8\n",
      " - 933s - loss: 0.2115 - acc: 0.9189 - val_loss: 0.2947 - val_acc: 0.8792\n",
      "Epoch 3/8\n",
      " - 1986s - loss: 0.1413 - acc: 0.9504 - val_loss: 0.3371 - val_acc: 0.8714\n",
      "Epoch 4/8\n",
      " - 820s - loss: 0.0938 - acc: 0.9703 - val_loss: 0.3813 - val_acc: 0.8674\n",
      "Epoch 5/8\n",
      " - 795s - loss: 0.0612 - acc: 0.9820 - val_loss: 0.4807 - val_acc: 0.8648\n",
      "Epoch 6/8\n",
      " - 791s - loss: 0.0401 - acc: 0.9886 - val_loss: 0.4891 - val_acc: 0.8658\n",
      "Epoch 7/8\n",
      " - 787s - loss: 0.0281 - acc: 0.9922 - val_loss: 0.6064 - val_acc: 0.8660\n",
      "Epoch 8/8\n",
      " - 793s - loss: 0.0184 - acc: 0.9959 - val_loss: 0.6856 - val_acc: 0.8618\n"
     ]
    }
   ],
   "source": [
    "embed_size2 = 300\n",
    "model = models.Sequential()\n",
    "model.add(Embedding(NB_WORDS, embed_size2))\n",
    "model.add(Bidirectional(LSTM(32, return_sequences = True)))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dense(20, activation=\"relu\"))\n",
    "model.add(Dropout(0.05))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "print(model.summary())\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "batch_size = 100\n",
    "epochs = 8\n",
    "fft2 = model.fit(X_train_seq_padded,y_train, batch_size=batch_size, epochs=epochs, \n",
    "          validation_data=(X_val_seq_padded, y_val), \n",
    "          verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#download data from https://nlp.stanford.edu/projects/glove/\n",
    "GLOVE_DIM = 300\n",
    "glove_file = 'glove.42B.' + str(GLOVE_DIM) + 'd.txt'\n",
    "emb_dict = {}\n",
    "glove = open(glove_file)\n",
    "for line in glove:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    vector = np.asarray(values[1:], dtype='float32')\n",
    "    emb_dict[word] = vector\n",
    "glove.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_matrix = np.zeros((NB_WORDS, GLOVE_DIM))\n",
    "\n",
    "for w, i in tk.word_index.items():\n",
    "    if i < NB_WORDS:\n",
    "        vect = emb_dict.get(w)\n",
    "        if vect is not None:\n",
    "            emb_matrix[i] = vect\n",
    "    else:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, None, 300)         1800000   \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection (None, None, 64)          85248     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_6 (Glob (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 20)                1300      \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 1,886,569\n",
      "Trainable params: 86,569\n",
      "Non-trainable params: 1,800,000\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "glove_model = models.Sequential()\n",
    "glove_model.add(Embedding(NB_WORDS, GLOVE_DIM, embeddings_initializer=Constant(emb_matrix), trainable = False))\n",
    "glove_model.add(Bidirectional(LSTM(32, return_sequences = True)))\n",
    "glove_model.add(GlobalMaxPool1D())\n",
    "glove_model.add(Dense(20, activation=\"relu\"))\n",
    "glove_model.add(Dropout(0.05))\n",
    "glove_model.add(Dense(1, activation=\"sigmoid\"))\n",
    "print(glove_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/5\n",
      " - 726s - loss: 0.1157 - acc: 0.9620 - val_loss: 0.3237 - val_acc: 0.8806\n",
      "Epoch 2/5\n",
      " - 546s - loss: 0.0864 - acc: 0.9755 - val_loss: 0.3432 - val_acc: 0.8746\n",
      "Epoch 3/5\n",
      " - 637s - loss: 0.0643 - acc: 0.9839 - val_loss: 0.3884 - val_acc: 0.8690\n",
      "Epoch 4/5\n",
      " - 545s - loss: 0.0440 - acc: 0.9908 - val_loss: 0.3932 - val_acc: 0.8780\n",
      "Epoch 5/5\n",
      " - 571s - loss: 0.0307 - acc: 0.9944 - val_loss: 0.4270 - val_acc: 0.8748\n"
     ]
    }
   ],
   "source": [
    "glove_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "batch_size = 100\n",
    "epochs = 5\n",
    "fit = glove_model.fit(X_train_seq_padded,y_train, batch_size=batch_size, epochs=epochs, \n",
    "          validation_data=(X_val_seq_padded, y_val), \n",
    "          verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, None, 300)         1800000   \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, None, 64)          85248     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_4 (Glob (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 20)                1300      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 1,887,409\n",
      "Trainable params: 87,409\n",
      "Non-trainable params: 1,800,000\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/12\n",
      " - 582s - loss: 0.4524 - acc: 0.7861 - val_loss: 0.3214 - val_acc: 0.8674\n",
      "Epoch 2/12\n",
      " - 575s - loss: 0.3068 - acc: 0.8745 - val_loss: 0.2992 - val_acc: 0.8764\n",
      "Epoch 3/12\n",
      " - 581s - loss: 0.2657 - acc: 0.8957 - val_loss: 0.3011 - val_acc: 0.8768\n",
      "Epoch 4/12\n",
      " - 578s - loss: 0.2271 - acc: 0.9140 - val_loss: 0.2930 - val_acc: 0.8802\n",
      "Epoch 5/12\n",
      " - 578s - loss: 0.1921 - acc: 0.9301 - val_loss: 0.3308 - val_acc: 0.8704\n",
      "Epoch 6/12\n",
      " - 582s - loss: 0.1597 - acc: 0.9452 - val_loss: 0.3287 - val_acc: 0.8758\n",
      "Epoch 7/12\n",
      " - 582s - loss: 0.1266 - acc: 0.9581 - val_loss: 0.3475 - val_acc: 0.8806\n",
      "Epoch 8/12\n",
      " - 597s - loss: 0.0996 - acc: 0.9690 - val_loss: 0.3708 - val_acc: 0.8742\n",
      "Epoch 9/12\n",
      " - 577s - loss: 0.0816 - acc: 0.9750 - val_loss: 0.4397 - val_acc: 0.8734\n",
      "Epoch 10/12\n",
      " - 576s - loss: 0.0689 - acc: 0.9782 - val_loss: 0.4189 - val_acc: 0.8736\n",
      "Epoch 11/12\n",
      " - 579s - loss: 0.0601 - acc: 0.9808 - val_loss: 0.4607 - val_acc: 0.8746\n",
      "Epoch 12/12\n",
      " - 585s - loss: 0.0389 - acc: 0.9885 - val_loss: 0.5081 - val_acc: 0.8744\n"
     ]
    }
   ],
   "source": [
    "glove_model = models.Sequential()\n",
    "glove_model.add(Embedding(NB_WORDS, GLOVE_DIM, embeddings_initializer=Constant(emb_matrix), trainable = False))\n",
    "glove_model.add(Bidirectional(LSTM(32, return_sequences = True)))\n",
    "glove_model.add(GlobalMaxPool1D())\n",
    "glove_model.add(Dense(20, activation=\"relu\"))\n",
    "glove_model.add(Dropout(0.05))\n",
    "glove_model.add(Dense(20, activation=\"relu\"))\n",
    "glove_model.add(Dropout(0.05))\n",
    "glove_model.add(Dense(20, activation=\"relu\"))\n",
    "glove_model.add(Dropout(0.05))\n",
    "glove_model.add(Dense(1, activation=\"sigmoid\"))\n",
    "print(glove_model.summary())\n",
    "\n",
    "glove_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "batch_size = 100\n",
    "epochs = 12\n",
    "fit2 = glove_model.fit(X_train_seq_padded,y_train, batch_size=batch_size, epochs=epochs, \n",
    "          validation_data=(X_val_seq_padded, y_val), \n",
    "          verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_files_to_df(path):\n",
    "    contents_dict = {}\n",
    "    files = glob.glob(path)\n",
    "    for file in files:\n",
    "        with open(file) as f:\n",
    "            base = os.path.basename(file)\n",
    "            id = os.path.splitext(base)[0]\n",
    "            contents_dict[id] = f.read()\n",
    "    return contents_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fit.on_batch_begin(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "mdl = fft2.model\n",
    "name = \"embed300\"\n",
    "\n",
    "model_json = mdl.to_json()\n",
    "with open(name + \".json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "mdl.save_weights(name+\".h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x17e005710>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x17e005710>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_files_to_dict(path):\n",
    "    contents_dict = {}\n",
    "    files = glob.glob(path)\n",
    "    for file in files:\n",
    "        with open(file) as f:\n",
    "            base = os.path.basename(file)\n",
    "            id = os.path.splitext(base)[0]\n",
    "            contents_dict[id] = f.read()\n",
    "    return contents_dict\n",
    "\n",
    "def prepare_test_data(test_data):\n",
    "    test_data_processed = []\n",
    "    for v in test_data.values():\n",
    "        test_data_processed += [remove_stopwords(v)]\n",
    "    test_seq = tk.texts_to_sequences(test_data_processed)\n",
    "    return pad_sequences(X_test_seq, maxlen=MAX_LEN)\n",
    "\n",
    "def create_submission_file(res_dir, keys, y_results):\n",
    "    with open(res_dir + '/submission.csv', 'w') as csv_file:\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "        csv_writer.writerow(['Id', 'Category'])\n",
    "        for key, result in zip(keys, y_results):\n",
    "            csv_writer.writerow([key, result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = read_files_to_dict('group34/data/test/*.txt')\n",
    "X_test_seq_padded = prepare_test_data(test_data)\n",
    "y_test_results = fit.model.predict(X_test_seq_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_results = []\n",
    "for res in y_test_results:\n",
    "    if (res < 0.5):\n",
    "        test_results += [0]\n",
    "    else:\n",
    "        test_results += [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "create_submission_file(\".\", test_data.keys(), test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00073427], dtype=float32)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_results[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test_results = classifier.predict(test_data.values())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
